{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# network.ipynb\n",
    "\n",
    "Contains the implementation and experimentation with neural networks from scratch (only numpy)\n",
    "\n",
    "## Design\n",
    "##### Network(sizes, activations, cost)\n",
    "- ex: `N = Network([10, 5, 7, 2], [relu, relu], cross_entropy_loss)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "def softmax(x):\n",
    "    exp = np.exp(x)\n",
    "    return exp / np.sum(exp, axis = 0)\n",
    "\n",
    "def cross_entropy_loss(Y_pred, Y_true):\n",
    "    return -np.sum(Y_true * np.log2(Y_pred)) #todo: test\n",
    "\n",
    "def cross_entropy_loss_derivative(Y_pred, Y_true):\n",
    "    return Y_pred - Y_true\n",
    "\n",
    "d = {relu: relu_derivative, cross_entropy_loss: cross_entropy_loss_derivative}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, sizes, activations, cost):\n",
    "        '''\n",
    "        Initialize all parameters in the network.\n",
    "        Weight matricies are initialized according to He et. al.\n",
    "        Bias vectors are initialized to 0.\n",
    "        '''\n",
    "        self.W = [ np.random.randn(sizes[l + 1], sizes[l]) * np.sqrt(2 / sizes[l]) for l in range(len(sizes) - 1) ]\n",
    "        self.b = [ np.zeros((size, 1)) for size in sizes[1:] ]\n",
    "        self.g = activations + [softmax]\n",
    "        self.cost = cost\n",
    "        \n",
    "    def dump(self):\n",
    "        for i, W, b, g in zip(range(len(self.W)), self.W, self.b, self.g):\n",
    "            print(f'Layer {i} W:{W.shape} b:{b.shape} g:{g}')\n",
    "        \n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        Feeds X forward through the network.\n",
    "        Returns (Z, A) lists of the unweighted and weighted activations in each layer.\n",
    "        '''\n",
    "        Z, A = [X], [X]\n",
    "        for W, b, g in zip(self.W, self.b, self.g):\n",
    "            Z.append(np.dot(W, A[-1]) + b)\n",
    "            A.append(g(Z[-1]))\n",
    "        return Z, A\n",
    "    \n",
    "    def backward(self, Z, A, Y):\n",
    "        '''\n",
    "        Computes derivatives for each parameter via backprop.\n",
    "        Z, A as in forward()\n",
    "        Y is one hot of true labels\n",
    "        Returns (dW, db) lists of the gradients of each weight matrix and bias vector.\n",
    "        '''\n",
    "        dW, db = [], []\n",
    "        dZ = cross_entropy_loss_derivative(A[-1], Y)\n",
    "        m = Y.shape[1]\n",
    "        for W, g, z, a in zip(self.W[::-1], self.g[-2::-1], Z[-2::-1], A[-2::-1]):\n",
    "            dW.insert(0, (1 / m) * np.dot(dZ, a.T))\n",
    "            db.insert(0, np.mean(dZ, axis = 1, keepdims = True))\n",
    "            dZ = np.dot(W.T, dZ) * d[g](z)\n",
    "        dW.insert(0, (1 / m) * np.dot(dZ, A[0].T))\n",
    "        db.insert(0, np.mean(dZ, axis = 1, keepdims = True))\n",
    "        return dW, db\n",
    "    \n",
    "    def fit(self, X, Y, alpha = 0.1, E = 100):\n",
    "        '''\n",
    "        Fits the network to the training data X, Y using gradient descent.\n",
    "        '''\n",
    "        for e in range(E):\n",
    "            Z, A = self.forward(X)\n",
    "            dW, db = self.backward(Z, A, Y)\n",
    "            self.W = [ w - alpha * g for w, g in zip(self.W, dW) ]\n",
    "            self.b = [ b - alpha * g for b, g in zip(self.b, db) ]\n",
    "            self.progress_update(A[-1], Y, e)\n",
    "            \n",
    "    def progress_update(self, A, Y, e):\n",
    "        cost = self.cost(A, Y)\n",
    "        error_rate = self.error_rate(A, Y)\n",
    "        print(f'epoch {e}: cost: {cost} error rate: {error_rate}')\n",
    "        \n",
    "    def error_rate(self, A, Y):\n",
    "        labels = np.argmax(Y, axis = 0)\n",
    "        predictions = np.argmax(A, axis = 0)\n",
    "        return np.mean(labels != predictions)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        Z, A = self.forward(X)\n",
    "        return np.argmax(A[-1], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: cost: 58.69303228172716 error rate: 0.56\n",
      "epoch 1: cost: 56.813883139902686 error rate: 0.54\n",
      "epoch 2: cost: 55.227585319462804 error rate: 0.58\n",
      "epoch 3: cost: 53.846895799154005 error rate: 0.52\n",
      "epoch 4: cost: 52.637475442098776 error rate: 0.52\n",
      "epoch 5: cost: 51.59483261040363 error rate: 0.5\n",
      "epoch 6: cost: 50.64803465428657 error rate: 0.5\n",
      "epoch 7: cost: 49.7999573761281 error rate: 0.5\n",
      "epoch 8: cost: 49.03803473807088 error rate: 0.5\n",
      "epoch 9: cost: 48.36242219073177 error rate: 0.46\n",
      "epoch 10: cost: 47.72877814722399 error rate: 0.46\n",
      "epoch 11: cost: 47.16898509095674 error rate: 0.42\n",
      "epoch 12: cost: 46.680255454142355 error rate: 0.44\n",
      "epoch 13: cost: 46.23688265777193 error rate: 0.42\n",
      "epoch 14: cost: 45.83187813221342 error rate: 0.38\n",
      "epoch 15: cost: 45.453443712989966 error rate: 0.36\n",
      "epoch 16: cost: 45.10176472508154 error rate: 0.36\n",
      "epoch 17: cost: 44.77134848795822 error rate: 0.32\n",
      "epoch 18: cost: 44.463631467819205 error rate: 0.32\n",
      "epoch 19: cost: 44.175202518409115 error rate: 0.32\n",
      "epoch 20: cost: 43.90077971032297 error rate: 0.32\n",
      "epoch 21: cost: 43.63742372663069 error rate: 0.32\n",
      "epoch 22: cost: 43.38878610666143 error rate: 0.32\n",
      "epoch 23: cost: 43.15033797354192 error rate: 0.32\n",
      "epoch 24: cost: 42.92189406578239 error rate: 0.32\n",
      "epoch 25: cost: 42.701781496896 error rate: 0.32\n",
      "epoch 26: cost: 42.48936540795013 error rate: 0.3\n",
      "epoch 27: cost: 42.29343447811818 error rate: 0.3\n",
      "epoch 28: cost: 42.10149654582162 error rate: 0.3\n",
      "epoch 29: cost: 41.916402522812085 error rate: 0.3\n",
      "epoch 30: cost: 41.739071963389215 error rate: 0.3\n",
      "epoch 31: cost: 41.565900310462276 error rate: 0.3\n",
      "epoch 32: cost: 41.3992949576361 error rate: 0.28\n",
      "epoch 33: cost: 41.23942943381181 error rate: 0.28\n",
      "epoch 34: cost: 41.084507920619764 error rate: 0.28\n",
      "epoch 35: cost: 40.93396178510663 error rate: 0.28\n",
      "epoch 36: cost: 40.785441683765725 error rate: 0.28\n",
      "epoch 37: cost: 40.64258790336771 error rate: 0.28\n",
      "epoch 38: cost: 40.50396030969652 error rate: 0.28\n",
      "epoch 39: cost: 40.369079202763366 error rate: 0.28\n",
      "epoch 40: cost: 40.236223211456654 error rate: 0.28\n",
      "epoch 41: cost: 40.107660041392755 error rate: 0.26\n",
      "epoch 42: cost: 39.981256814858575 error rate: 0.26\n",
      "epoch 43: cost: 39.85715125893985 error rate: 0.26\n",
      "epoch 44: cost: 39.73612755429728 error rate: 0.26\n",
      "epoch 45: cost: 39.61778739821132 error rate: 0.26\n",
      "epoch 46: cost: 39.50259546390431 error rate: 0.26\n",
      "epoch 47: cost: 39.388088070993504 error rate: 0.26\n",
      "epoch 48: cost: 39.27713962566165 error rate: 0.26\n",
      "epoch 49: cost: 39.16984430766682 error rate: 0.26\n",
      "epoch 50: cost: 39.06412265882725 error rate: 0.26\n",
      "epoch 51: cost: 38.96245896734796 error rate: 0.26\n",
      "epoch 52: cost: 38.858184290717105 error rate: 0.26\n",
      "epoch 53: cost: 38.756001841567816 error rate: 0.26\n",
      "epoch 54: cost: 38.65609813260928 error rate: 0.26\n",
      "epoch 55: cost: 38.55841496652645 error rate: 0.26\n",
      "epoch 56: cost: 38.461955773567894 error rate: 0.26\n",
      "epoch 57: cost: 38.367018541200096 error rate: 0.26\n",
      "epoch 58: cost: 38.27515446618747 error rate: 0.26\n",
      "epoch 59: cost: 38.184919150413364 error rate: 0.26\n",
      "epoch 60: cost: 38.0966298050821 error rate: 0.26\n",
      "epoch 61: cost: 38.00644759978107 error rate: 0.26\n",
      "epoch 62: cost: 37.9193227481081 error rate: 0.26\n",
      "epoch 63: cost: 37.83012601131188 error rate: 0.26\n",
      "epoch 64: cost: 37.74104977229593 error rate: 0.26\n",
      "epoch 65: cost: 37.65278706142068 error rate: 0.26\n",
      "epoch 66: cost: 37.564807719052595 error rate: 0.26\n",
      "epoch 67: cost: 37.47857309544106 error rate: 0.26\n",
      "epoch 68: cost: 37.39157777727329 error rate: 0.26\n",
      "epoch 69: cost: 37.308205047136255 error rate: 0.26\n",
      "epoch 70: cost: 37.22605900788207 error rate: 0.26\n",
      "epoch 71: cost: 37.14032823749906 error rate: 0.26\n",
      "epoch 72: cost: 37.05964674471297 error rate: 0.26\n",
      "epoch 73: cost: 36.97750746855935 error rate: 0.26\n",
      "epoch 74: cost: 36.89415592438365 error rate: 0.26\n",
      "epoch 75: cost: 36.81205361346206 error rate: 0.26\n",
      "epoch 76: cost: 36.72969050460116 error rate: 0.26\n",
      "epoch 77: cost: 36.65112964964073 error rate: 0.26\n",
      "epoch 78: cost: 36.575594585174976 error rate: 0.26\n",
      "epoch 79: cost: 36.50204884962061 error rate: 0.26\n",
      "epoch 80: cost: 36.42582456233135 error rate: 0.26\n",
      "epoch 81: cost: 36.351608432478244 error rate: 0.26\n",
      "epoch 82: cost: 36.27794459987239 error rate: 0.26\n",
      "epoch 83: cost: 36.20470961865565 error rate: 0.26\n",
      "epoch 84: cost: 36.130776977040064 error rate: 0.26\n",
      "epoch 85: cost: 36.05915139840022 error rate: 0.26\n",
      "epoch 86: cost: 35.98632847415265 error rate: 0.26\n",
      "epoch 87: cost: 35.91289074727691 error rate: 0.26\n",
      "epoch 88: cost: 35.84193246067236 error rate: 0.26\n",
      "epoch 89: cost: 35.77124868345828 error rate: 0.26\n",
      "epoch 90: cost: 35.70066612019395 error rate: 0.26\n",
      "epoch 91: cost: 35.632783539704654 error rate: 0.26\n",
      "epoch 92: cost: 35.568462757585266 error rate: 0.26\n",
      "epoch 93: cost: 35.502250588001516 error rate: 0.26\n",
      "epoch 94: cost: 35.43472046816487 error rate: 0.26\n",
      "epoch 95: cost: 35.37008887199972 error rate: 0.26\n",
      "epoch 96: cost: 35.306988961377996 error rate: 0.26\n",
      "epoch 97: cost: 35.2424229651411 error rate: 0.24\n",
      "epoch 98: cost: 35.179218872088825 error rate: 0.24\n",
      "epoch 99: cost: 35.11659392564297 error rate: 0.24\n"
     ]
    }
   ],
   "source": [
    "N = Network([3, 50, 50, 50, 50, 2], [relu] * 4, cross_entropy_loss)\n",
    "X = np.random.randn(3, 50)\n",
    "\n",
    "I = np.random.randint(0, 2, size = 50)\n",
    "Y = np.eye(2)[I].T\n",
    "N.fit(X, Y, 0.01, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(X_train_orig, Y_train_orig), (X_test_orig, Y_test_orig) = mnist.load_data()\n",
    "print(X_train_orig.shape)\n",
    "print(Y_train_orig.shape)\n",
    "print(X_test_orig.shape)\n",
    "print(Y_test_orig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_orig.reshape(60000, -1).T / 255\n",
    "X_test = X_test_orig.reshape(10000, -1).T / 255\n",
    "Y_train = np.eye(10)[Y_train_orig].T\n",
    "Y_test = np.eye(10)[Y_test_orig].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd362046510>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(X_train[:, 1].reshape(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: cost: 34.3412489265195 error rate: 1.0\n",
      "epoch 1: cost: 33.08105066554364 error rate: 0.9\n",
      "epoch 2: cost: 31.97872233130015 error rate: 0.8\n",
      "epoch 3: cost: 30.978150941988165 error rate: 0.7\n",
      "epoch 4: cost: 30.068897756105372 error rate: 0.6\n",
      "epoch 5: cost: 29.209816293334065 error rate: 0.6\n",
      "epoch 6: cost: 28.397703036588492 error rate: 0.6\n",
      "epoch 7: cost: 27.59948410099374 error rate: 0.6\n",
      "epoch 8: cost: 26.863887514066565 error rate: 0.6\n",
      "epoch 9: cost: 26.128254823943823 error rate: 0.5\n",
      "epoch 10: cost: 25.411226166588428 error rate: 0.5\n",
      "epoch 11: cost: 24.74021925869646 error rate: 0.3\n",
      "epoch 12: cost: 24.087631192905025 error rate: 0.3\n",
      "epoch 13: cost: 23.46627179165018 error rate: 0.2\n",
      "epoch 14: cost: 22.854007689031143 error rate: 0.2\n",
      "epoch 15: cost: 22.262725558855017 error rate: 0.2\n",
      "epoch 16: cost: 21.677055455037998 error rate: 0.1\n",
      "epoch 17: cost: 21.114615753902996 error rate: 0.1\n",
      "epoch 18: cost: 20.58316385800431 error rate: 0.1\n",
      "epoch 19: cost: 20.01883686720485 error rate: 0.1\n",
      "epoch 20: cost: 19.472327288763466 error rate: 0.1\n",
      "epoch 21: cost: 18.965649943835892 error rate: 0.1\n",
      "epoch 22: cost: 18.432506129835737 error rate: 0.1\n",
      "epoch 23: cost: 17.91360682004806 error rate: 0.1\n",
      "epoch 24: cost: 17.41824319076767 error rate: 0.1\n",
      "epoch 25: cost: 16.92298651596949 error rate: 0.1\n",
      "epoch 26: cost: 16.44483605225342 error rate: 0.1\n",
      "epoch 27: cost: 15.971591013111519 error rate: 0.1\n",
      "epoch 28: cost: 15.5185405923568 error rate: 0.1\n",
      "epoch 29: cost: 15.07571036338501 error rate: 0.1\n",
      "epoch 30: cost: 14.649569056603761 error rate: 0.1\n",
      "epoch 31: cost: 14.234294035065869 error rate: 0.1\n",
      "epoch 32: cost: 13.833643857587248 error rate: 0.1\n",
      "epoch 33: cost: 13.44213449038906 error rate: 0.1\n",
      "epoch 34: cost: 13.059494485376193 error rate: 0.1\n",
      "epoch 35: cost: 12.679560553202837 error rate: 0.1\n",
      "epoch 36: cost: 12.32582103653634 error rate: 0.0\n",
      "epoch 37: cost: 11.977661238897783 error rate: 0.0\n",
      "epoch 38: cost: 11.629693442861312 error rate: 0.0\n",
      "epoch 39: cost: 11.303571176018952 error rate: 0.0\n",
      "epoch 40: cost: 10.974031220830483 error rate: 0.0\n",
      "epoch 41: cost: 10.669531084415762 error rate: 0.0\n",
      "epoch 42: cost: 10.359222022894865 error rate: 0.0\n",
      "epoch 43: cost: 10.067738680134582 error rate: 0.0\n",
      "epoch 44: cost: 9.759129611613957 error rate: 0.0\n",
      "epoch 45: cost: 9.479468214447703 error rate: 0.0\n",
      "epoch 46: cost: 9.199336638410529 error rate: 0.0\n",
      "epoch 47: cost: 8.940476749644741 error rate: 0.0\n",
      "epoch 48: cost: 8.673800795700632 error rate: 0.0\n",
      "epoch 49: cost: 8.41889080855004 error rate: 0.0\n",
      "epoch 50: cost: 8.170915659178224 error rate: 0.0\n",
      "epoch 51: cost: 7.9266821708709365 error rate: 0.0\n",
      "epoch 52: cost: 7.685112071318558 error rate: 0.0\n",
      "epoch 53: cost: 7.4539029697249335 error rate: 0.0\n",
      "epoch 54: cost: 7.232367227955001 error rate: 0.0\n",
      "epoch 55: cost: 7.015738198211233 error rate: 0.0\n",
      "epoch 56: cost: 6.80723006613954 error rate: 0.0\n",
      "epoch 57: cost: 6.609619504639129 error rate: 0.0\n",
      "epoch 58: cost: 6.417246955510468 error rate: 0.0\n",
      "epoch 59: cost: 6.236298036246142 error rate: 0.0\n",
      "epoch 60: cost: 6.062820863975324 error rate: 0.0\n",
      "epoch 61: cost: 5.896237012088283 error rate: 0.0\n",
      "epoch 62: cost: 5.734539592782028 error rate: 0.0\n",
      "epoch 63: cost: 5.581426316509191 error rate: 0.0\n",
      "epoch 64: cost: 5.429560529737003 error rate: 0.0\n",
      "epoch 65: cost: 5.291500049909955 error rate: 0.0\n",
      "epoch 66: cost: 5.150635133745008 error rate: 0.0\n",
      "epoch 67: cost: 5.0162024938208685 error rate: 0.0\n",
      "epoch 68: cost: 4.888219396162163 error rate: 0.0\n",
      "epoch 69: cost: 4.764739268882006 error rate: 0.0\n",
      "epoch 70: cost: 4.647317028485986 error rate: 0.0\n",
      "epoch 71: cost: 4.530119478943567 error rate: 0.0\n",
      "epoch 72: cost: 4.417528543475552 error rate: 0.0\n",
      "epoch 73: cost: 4.312102734610516 error rate: 0.0\n",
      "epoch 74: cost: 4.206330639454103 error rate: 0.0\n",
      "epoch 75: cost: 4.10633034689209 error rate: 0.0\n",
      "epoch 76: cost: 4.008444415400398 error rate: 0.0\n",
      "epoch 77: cost: 3.913712245875144 error rate: 0.0\n",
      "epoch 78: cost: 3.8225250512671343 error rate: 0.0\n",
      "epoch 79: cost: 3.7353057531061573 error rate: 0.0\n",
      "epoch 80: cost: 3.6491283311082947 error rate: 0.0\n",
      "epoch 81: cost: 3.5680042916972448 error rate: 0.0\n",
      "epoch 82: cost: 3.488672146975061 error rate: 0.0\n",
      "epoch 83: cost: 3.4108323733339687 error rate: 0.0\n",
      "epoch 84: cost: 3.3364638855936777 error rate: 0.0\n",
      "epoch 85: cost: 3.265530577205848 error rate: 0.0\n",
      "epoch 86: cost: 3.1951070982784135 error rate: 0.0\n",
      "epoch 87: cost: 3.1296586529089323 error rate: 0.0\n",
      "epoch 88: cost: 3.0622181407459252 error rate: 0.0\n",
      "epoch 89: cost: 2.999647518049711 error rate: 0.0\n",
      "epoch 90: cost: 2.937590113579221 error rate: 0.0\n",
      "epoch 91: cost: 2.8800698026117333 error rate: 0.0\n",
      "epoch 92: cost: 2.8205053415289685 error rate: 0.0\n",
      "epoch 93: cost: 2.7654857500650705 error rate: 0.0\n",
      "epoch 94: cost: 2.7107982721942685 error rate: 0.0\n",
      "epoch 95: cost: 2.6601496596256 error rate: 0.0\n",
      "epoch 96: cost: 2.6076998365995347 error rate: 0.0\n",
      "epoch 97: cost: 2.5588244049345965 error rate: 0.0\n",
      "epoch 98: cost: 2.5106086231600617 error rate: 0.0\n",
      "epoch 99: cost: 2.464226307072277 error rate: 0.0\n",
      "[5 0 4 ... 1 4 4]\n",
      "[5 0 4 ... 5 6 8]\n"
     ]
    }
   ],
   "source": [
    "N = Network([784, 64, 64, 10], [relu] * 2, cross_entropy_loss)\n",
    "N.fit(X_train[:, :10], Y_train[:, :10], 0.01, 100)\n",
    "predictions = N.predict(X_train)\n",
    "print(predictions)\n",
    "print(Y_train_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
